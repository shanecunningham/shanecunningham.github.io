<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>openstack on Computers</title>
    <link>https://shanecunningham.github.io/categories/openstack/</link>
    <description>Recent content in openstack on Computers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Copyright 2021, Shane Cunningham</copyright>
    <lastBuildDate>Fri, 19 Aug 2016 09:37:00 +0000</lastBuildDate><atom:link href="https://shanecunningham.github.io/categories/openstack/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deploying and customizing OpenStack Mitaka with openstack-ansible</title>
      <link>https://shanecunningham.github.io/posts/deploying-and-customizing-openstack-mitaka-with-openstack-ansible/</link>
      <pubDate>Fri, 19 Aug 2016 09:37:00 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/deploying-and-customizing-openstack-mitaka-with-openstack-ansible/</guid>
      <description>This guide will be similar to my other guides on how to install OpenStack using openstack-ansible, LXC containers and some simple YAML configs, but I plan to go a little more in depth with some of the configuration options and customizations that are available. This version will deploy OpenStack Mitaka.
Overview  Hardware Setting up physical hosts Downloading openstack-ansible Customizing our OpenStack cloud Installing OpenStack Configuring Neutron Testing our cloud Next  Hardware infra01: Lenovo ThinkServer TS140 Xeon E3-1225 v3 3.</description>
    </item>
    
    <item>
      <title>Deploying OpenStack Liberty with Ceph</title>
      <link>https://shanecunningham.github.io/posts/deploying-openstack-and-ceph/</link>
      <pubDate>Sun, 10 Apr 2016 00:17:12 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/deploying-openstack-and-ceph/</guid>
      <description>In this example of deploying OpenStack I&amp;rsquo;ll be adding a third server that will act as our Ceph storage server. With a few config changes to openstack-ansible we will setup nova, cinder and glance to use Ceph as their backend storage systems.
infra01: Lenovo ThinkServer TS140 Xeon E3-1225 v3 3.2 GHz 16GB ECC RAM 2 x 1Gb NICs
IP address for em1: 192.168.88.100 IP address for br-mgmt: 172.29.236.51 IP address for br-vxlan: 172.</description>
    </item>
    
    <item>
      <title>Deploying OpenStack Kilo with openstack-ansible</title>
      <link>https://shanecunningham.github.io/posts/deploying-openstack-kilo-with-openstack-ansible/</link>
      <pubDate>Sun, 18 Oct 2015 04:43:18 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/deploying-openstack-kilo-with-openstack-ansible/</guid>
      <description>openstack-ansible is an open source project started by Rackspace to make deploying large OpenStack clouds easier. You describe your OpenStack environment in configuration files and openstack-ansible uses Ansible to lay down OpenStack from source and runs most OpenStack services inside LXC containers.
The Kilo version of openstack-ansible was released last month. With that release came some significant changes from the Icehouse/Juno openstack-ansible code. This started as a Rackspace project, the Icehouse and Juno versions have Rackspace specific bits inside.</description>
    </item>
    
    <item>
      <title>Tips for managing OpenStack with openstack-ansible</title>
      <link>https://shanecunningham.github.io/posts/tips-for-managing-openstack-with-openstack-ansible/</link>
      <pubDate>Sat, 18 Jul 2015 22:56:06 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/tips-for-managing-openstack-with-openstack-ansible/</guid>
      <description>openstack-ansible (OSAD), is a deployment method that uses Ansible to lay down OpenStack inside LXC containers. It makes deploying large OpenStack deployments easier. Deploying most components of OpenStack inside LXC containers also makes upgrading as easy as downloading the new playbooks and running.
Here are some tips on managing the cluster once you&amp;rsquo;re up and runnning. Since the inventory includes all your hosts and containers, you can use Ansible to manage OpenStack and system administration tasks.</description>
    </item>
    
    <item>
      <title>Deploying OpenStack in containers: Install and Upgrade</title>
      <link>https://shanecunningham.github.io/posts/openstack-in-containers-install-and-upgrade/</link>
      <pubDate>Sat, 21 Mar 2015 11:01:33 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/openstack-in-containers-install-and-upgrade/</guid>
      <description>My post on using openstack-ansible to deploy OpenStack in LXC containers in a two-node configuration with two NICs. One NIC is used for management, API and VM to VM traffic, the other NIC is for external network access. This is just for testing and messing around with deploying OpenStack in containers. An advantage to deploying with Ansible and containers is the easier upgrade path it provides. I&amp;rsquo;ll show a simple example of that in place upgrade with going from Icehouse to Juno with just running a few playbooks.</description>
    </item>
    
    <item>
      <title>OpenStack Swift and Cyberduck</title>
      <link>https://shanecunningham.github.io/posts/openstack-swift-and-cyberduck/</link>
      <pubDate>Fri, 13 Mar 2015 09:43:35 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/openstack-swift-and-cyberduck/</guid>
      <description>Just a couple notes. I recently added a Swift node to my OpenStack deployment in my closet. For some reason I kept wanting to point Cyberduck to the infra1_swift_proxy_container at port 8080. Instead, you want to point it to port 5000 for Keystone auth, duh. By default SSL is not enabled in Swift so you can download the HTTP OpenStack profile OpenstackSwift(HTTP).cyberduckprofile. &amp;ldquo;Username:&amp;rdquo; in Cyberduck wants tenant:user, so out of the box you could use admin:admin, and the &amp;ldquo;Secret Key&amp;rdquo; would be the admin users password.</description>
    </item>
    
    <item>
      <title>RPC v9 - Two node with floating IPs</title>
      <link>https://shanecunningham.github.io/posts/rpc-v9-two-node-installation-with-floating-ips/</link>
      <pubDate>Sat, 07 Feb 2015 05:29:57 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/rpc-v9-two-node-installation-with-floating-ips/</guid>
      <description>Building on my previous post on installing Rackspace Private Cloud in a two node configuration, this update changes that slightly to allow for external network access and floating IPs to work with this deployment. My previous post used all VXLAN interfaces from a single physical NIC. That would not allow external/floating IPs to work, or at least I couldn&amp;rsquo;t get it to work. Since I have two physical NICs on these servers this guide will use both - em1 for br-mgmt, br-vxlan, br-storage via VXLAN interfaces since I don&amp;rsquo;t want to use VLANs, and p4p1 for br-vlan, which has direct access to my external network, 192.</description>
    </item>
    
    <item>
      <title>Two node RPC v9 installation</title>
      <link>https://shanecunningham.github.io/posts/how-to-rpc-v9-two-node-installation/</link>
      <pubDate>Sun, 11 Jan 2015 05:16:25 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/how-to-rpc-v9-two-node-installation/</guid>
      <description>Rackspace Private Cloud powered by OpenStack was recently re-architected to be much more flexible and reliable in RPC v9 (Icehouse) and the soon to be released RPC v10 (Juno). It actually deploys OpenStack in LXC containers on your hosts. At first, you might think this adds a layer of complexity in an already complex process, but I&amp;rsquo;ve found it actually provides a tremendous amount of flexibility and an easier upgrade path for your OpenStack installation.</description>
    </item>
    
    <item>
      <title>OpenStack Juno All in One</title>
      <link>https://shanecunningham.github.io/posts/openstack-juno-all-in-one/</link>
      <pubDate>Sat, 08 Nov 2014 11:32:31 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/openstack-juno-all-in-one/</guid>
      <description>Quick guide on setting up the OpenStack Juno release in an all in one server with one NIC using RDO. This is configured for Neutron networking with floating IPs.
My setup: CentOS 7 minimal, IP: 192.168.1.100
By default NetworkManager will be running and controlling our NICs. packstack will complain about this later so disable and stop the service.
Next we&amp;rsquo;ll update some stuff, install the RDO Juno repo and install packstack.</description>
    </item>
    
    <item>
      <title>Migrating Cinder volumes to Icehouse</title>
      <link>https://shanecunningham.github.io/posts/migrating-cinder-volumes-to-icehouse/</link>
      <pubDate>Fri, 09 May 2014 07:20:18 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/migrating-cinder-volumes-to-icehouse/</guid>
      <description>I upgraded my all in one OpenStack Havana box to the new Icehouse release. All the same steps apply as in my all in one OpenStack deployment post except use http://rdo.fedorapeople.org/rdo-release.rpm which now redirects to the Icehouse RPM. I wanted to blow everything away and not perform an upgrade. The only issue I ran into was I boot my VMs with Cinder volumes for persistant storage (just use LVM to create a volume group named &amp;ldquo;cinder-volumes&amp;rdquo;) and I wanted to move those to Icehouse with the data untouched.</description>
    </item>
    
    <item>
      <title>My all in one OpenStack deployment at home</title>
      <link>https://shanecunningham.github.io/posts/my-all-in-one-openstack-deployment-at-home/</link>
      <pubDate>Sun, 19 Jan 2014 22:33:09 +0000</pubDate>
      
      <guid>https://shanecunningham.github.io/posts/my-all-in-one-openstack-deployment-at-home/</guid>
      <description>I use XenServer 6.2 as my hypervisor at home to run anywhere from 5-10 VMs. But I wanted to change up this setup and move to OpenStackPrivate Cloud deployment. Yes, it&amp;rsquo;s overkill for my use but oh well.
I&amp;rsquo;ve messed around a few times with using OpenStack as replacement for my XenServer 6.2 setup, but always ran into an issue, usually getting the networking correct given my home network. Luckily with the OpenStack Havana release networking has become much simpler to get my head around and deploy.</description>
    </item>
    
  </channel>
</rss>
