<!doctype html>
<html lang="en"><head>
    <title>Deploying OpenStack Liberty with Ceph</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="" />

    
    
    
    <link rel="stylesheet" href="../../../css/theme.min.css">

    
    
    

    
</head>
<body>
        <div id="content" class="mx-auto"><header class="container mt-sm-5 mt-4 mb-4 mt-xs-1">
    <div class="row">
        <div class="col-sm-4 col-12 text-sm-right text-center pt-sm-4">
            <a href="../../../" class="text-decoration-none">
                <img id="home-image" class="rounded-circle"
                    
                        src="../../../images/avatar.png"
                    
                />
            </a>
        </div>
        <div class="col-sm-8 col-12 text-sm-left text-center">
            <h2 class="m-0 mb-2 mt-4">
                <a href="../../../" class="text-decoration-none">
                    
                        Computers
                    
                </a>
            </h2>
            <p class="text-muted mb-1">
                
                    
                
            </p>
            <ul id="nav-links" class="list-inline mb-2">
                
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../../about/" title="About">About</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../../posts/" title="Posts">Posts</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../../posts/archive/" title="Archives">Archives</a>
                    </li>
                
            </ul>
            <ul id="nav-social" class="list-inline">
                
                    <li class="list-inline-item mr-3">
                        <a href="http://github.com/shanecunningham" target="_blank">
                            <i class="fab fa-github fa-1x text-muted"></i>
                        </a>
                    </li>
                
                    <li class="list-inline-item mr-3">
                        <a href="" target="_blank">
                            <i class="fab fa-linkedin-in fa-1x text-muted"></i>
                        </a>
                    </li>
                
                    <li class="list-inline-item mr-3">
                        <a href="" target="_blank">
                            <i class="fab fa-twitter fa-1x text-muted"></i>
                        </a>
                    </li>
                
                    <li class="list-inline-item mr-3">
                        <a href="" target="_blank">
                            <i class="fas fa-at fa-1x text-muted"></i>
                        </a>
                    </li>
                
            </ul>
        </div>
    </div>
    <hr />
</header>
<div class="container">
    <div class="pl-sm-4 ml-sm-5">
        <p>In this example of deploying OpenStack I&rsquo;ll be adding a third server that will act as our <a href="http://ceph.com/">Ceph</a> storage server. With a few config changes to openstack-ansible we will setup nova, cinder and glance to use Ceph as their backend storage systems.</p>
<p><strong>infra01</strong>:
Lenovo ThinkServer TS140
Xeon E3-1225 v3 3.2 GHz
16GB ECC RAM
2 x 1Gb NICs</p>
<pre><code>IP address for em1:        192.168.88.100  
IP address for br-mgmt:    172.29.236.51  
IP address for br-vxlan:   172.29.240.51  
IP address for br-storage: 172.29.244.51  
IP address for br-vlan:    none  
</code></pre><p><strong>compute01</strong>:
Dell Poweredge T110 II
Xeon E3-1230 v2 3.3 GHz
32GB ECC RAM
2 x 1Gb NICs</p>
<pre><code>IP address for em1:        192.168.88.101  
IP address for br-mgmt:    172.29.236.52  
IP address for br-vxlan:   172.29.240.52  
IP address for br-storage: 172.29.244.52  
IP address for br-vlan:    none
</code></pre><p><strong>object01</strong>:
Dell Poweredge T20
Intel Pentium G3220 3.0 GHz
4GB ECC RAM
2 x 1Gb NICs
3 x 2TB SATA drives for Ceph storage</p>
<pre><code>IP address for em1:        192.168.88.102  
IP address for br-mgmt:    172.29.236.53  
IP address for br-vxlan:   172.29.240.53  
IP address for br-storage: 172.29.244.53  
IP address for br-vlan:    none
</code></pre><p>I&rsquo;ll jump right into the configuration for Ceph. This uses rpc-openstack (tag r12.0.0) which is openstack-ansible wrapped with some helpful additions like installing ceph for us. If you have questions on a more step by step openstack-ansible Kilo install, see my other posts.</p>
<p>We need to add the Ceph environment file to /etc/openstack_deploy/env.d/ so openstack-ansible knows we want to deploy Ceph.</p>
<pre><code>cp -a /opt/rpc-openstack/rpcd/etc/openstack_deploy/env.d/ceph.yml /etc/openstack_deploy/env.d
</code></pre><p>Create our <a href="https://gist.github.com/shane-c/bf629d203fccd25bd12dd678720ecbca">/etc/openstack_deploy/conf.d/ceph.yml</a> file which describes our specific Ceph deployment. We create 3 mons containers that live on the infrastructure host along with our other containers. We also tell Ansible where our OSD hosts are and the disks we&rsquo;ll be using for ceph storage. And the last storage_hosts section is where we tell cinder to use ceph for its backend storage for volumes.</p>
<pre><code>---
mons_hosts:
  infra01:
    ip: 192.168.88.100
    affinity:
      ceph_mon_container: 3

osds_hosts:
  object01:
    ip: 192.168.88.102
    affinity:
      ceph_osd_container: 3
    container_vars:
    # The devices must be specified in conjunction with raw_journal_devices below.
    # If you want one journal device per five drives, specify the same journal_device five times.
      devices:
         - /dev/sda
         - /dev/sdb
         - /dev/sdc
    # Note: If a device does not have a matching &quot;raw_journal_device&quot;,
    #       it will co-locate the journal on the device.
      raw_journal_devices:
        - /dev/sda
        - /dev/sdb
        - /dev/sdc

storage_hosts:
  infra01:
    ip: 192.168.88.100
    container_vars:
      cinder_backends:
        limit_container_types: cinder_volume
        ceph:
          volume_driver: cinder.volume.drivers.rbd.RBDDriver
          rbd_pool: volumes
          rbd_ceph_conf: /etc/ceph/ceph.conf
          rbd_flatten_volume_from_snapshot: 'false'
          rbd_max_clone_depth: 5
          rbd_store_chunk_size: 4
          rados_connect_timeout: -1
          glance_api_version: 2
          volume_backend_name: ceph
          rbd_user: &quot;{{ cinder_ceph_client }}&quot;
          rbd_secret_uuid: &quot;{{ cinder_ceph_client_uuid }}&quot;
</code></pre><p>If we are only using Ceph as the backend for Cinder (not using another host with LVM or any other backend) then we can containerize the cinder-volumes process.</p>
<pre><code>sed -i 's/is_metal: true/is_metal: false/g' /etc/openstack_deploy/env.d/cinder.yml
</code></pre><p>We need our mons containers to have an interface in the br-storage network so we need to add it to the group_binds in our br-storage provider_network section in openstack_user_config.yml file.</p>
<pre><code>    - network:
        container_bridge: &quot;br-storage&quot;
        container_type: &quot;veth&quot;
        container_interface: &quot;eth2&quot;
        ip_from_q: &quot;storage&quot;
        type: &quot;raw&quot;
        group_binds:
          - glance_api
          - cinder_api
          - cinder_volume
          - nova_compute
          - mons
</code></pre><p>Add these variables to the end of /etc/openstack_deploy/user_extra_variables.yml. <strong>Note</strong>: common_single_host_mode is only required in this demo because I&rsquo;m basically doing an all in one OSD host with only 3 disks.</p>
<pre><code>monitor_interface: 'eth1'
public_network: '172.29.236.0/22' # This is the network from external -&gt; storage (and mons).
cluster_network: '172.29.244.0/22' # This is OSD to OSD replication.
common_single_host_mode: true
</code></pre><p>Add the following in /etc/openstack_deploy/user_variables.yml.</p>
<pre><code>glance_default_store: rbd
nova_libvirt_images_rbd_pool: vms
nova_force_config_drive: False
nova_nova_conf_overrides:
  libvirt:
    live_migration_uri: qemu+ssh://nova@%s/system?keyfile=/var/lib/nova/.ssh/id_rsa&amp;no_verify=1
</code></pre><p>We can now start running playbooks.</p>
<pre><code># openstack-ansible setup-hosts.yml
# openstack-ansible haproxy-install.yml
# cd ../../rpcd/playbooks
# openstack-ansible ceph-all.yml
</code></pre><p>At this point we should have a working Ceph installation. lxc-attach to one of the ceph_mon container on the infrastructure host and you should be able to run ceph -s and ceph df to check the status of ceph. Continue running setup-infrastructure.yml and setup-openstack.yml to complete the install.</p>
<pre><code>root@infra01_ceph_mon_container-5e4cfc8d:/# ceph -s
    cluster db518fe7-6dc1-4a52-a2fc-86a4f0890f4d
     health HEALTH_OK
     monmap e1: 3 mons at {infra01_ceph_mon_container-5e4cfc8d=172.29.236.251:6789/0,infra01_ceph_mon_container-a5c1bfe6=172.29.236.191:6789/0,infra01_ceph_mon_container-c5e7ea12=172.29.238.198:6789/0}
            election epoch 8, quorum 0,1,2 infra01_ceph_mon_container-a5c1bfe6,infra01_ceph_mon_container-5e4cfc8d,infra01_ceph_mon_container-c5e7ea12
     osdmap e35: 3 osds: 3 up, 3 in
      pgmap v74: 512 pgs, 4 pools, 0 bytes data, 0 objects
            117 MB used, 5351 GB / 5352 GB avail
                 512 active+clean

root@infra01_ceph_mon_container-5e4cfc8d:/# ceph df
GLOBAL:
    SIZE      AVAIL     RAW USED     %RAW USED
    5352G     5351G         117M             0
POOLS:
    NAME        ID     USED     %USED     MAX AVAIL     OBJECTS
    images      1         0         0         1783G           0
    volumes     2         0         0         1783G           0
    vms         3         0         0         1783G           0
    backups     4         0         0         1783G           0
</code></pre><p>This is after adding a glance image, spinning up two instances and creating one cinder volume.</p>
<pre><code>root@infra01_ceph_mon_container-5e4cfc8d:/# ceph -s
    cluster db518fe7-6dc1-4a52-a2fc-86a4f0890f4d
     health HEALTH_OK
     monmap e1: 3 mons at {infra01_ceph_mon_container-5e4cfc8d=172.29.236.251:6789/0,infra01_ceph_mon_container-a5c1bfe6=172.29.236.191:6789/0,infra01_ceph_mon_container-c5e7ea12=172.29.238.198:6789/0}
            election epoch 8, quorum 0,1,2 infra01_ceph_mon_container-a5c1bfe6,infra01_ceph_mon_container-5e4cfc8d,infra01_ceph_mon_container-c5e7ea12
     osdmap e42: 3 osds: 3 up, 3 in
      pgmap v257: 512 pgs, 4 pools, 2572 MB data, 384 objects
            7673 MB used, 5344 GB / 5352 GB avail
                 512 active+clean
  client io 61688 kB/s rd, 0 B/s wr, 422 op/s

root@infra01_ceph_mon_container-5e4cfc8d:/# ceph df
GLOBAL:
    SIZE      AVAIL     RAW USED     %RAW USED
    5352G     5344G        7673M          0.14
POOLS:
    NAME        ID     USED      %USED     MAX AVAIL     OBJECTS
    images      1      2252M      0.04         1781G         285
    volumes     2         16         0         1781G           3
    vms         3       320M         0         1781G          96
    backups     4          0         0         1781G           0
</code></pre><p>With <a href="https://github.com/openstack/openstack-ansible">openstack-ansible</a> and <a href="https://github.com/rcbops/rpc-openstack">rpc-openstack</a>, getting an OpenStack cluster up and running with Ceph as the backend storage couldn&rsquo;t be easier.</p>

    </div>

            </div>
        </div><footer class="text-center pb-1">
    <small class="text-muted">
        
            &copy; Copyright 2021, Shane Cunningham
        
        <br>
        Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
        and <a href="https://github.com/austingebauer/devise" target="_blank">Devise</a>
    </small>
</footer>
</body>
</html>
